= Jamal OpenAI module




== Introduction

With the OpenAI module you can include text generated by the OpenAI API into your Jamal documents.
That way you can ask OpenAI to complete your text, answer questions and so on.
The completed text or picture has to be reviewed.
You can use the OpenAI module macros to seal the results.
Sealed results are stored in cache files and do not change, and also do not require any API calls.
That way you can freely change the parts of the document, which do not depend on the OpenAI API results without needing to call the API again.

Calling OpenAI API is not free and can be abused.
Therefore, the OpenAI module is an unsafe module.
It means that the Jamal document has to load it using the `maven:load` macro and the module has to be configured on your machine as trusted.
Starting to use this module is not simple.

The OpenAI API is also complex adding to the complexity of the module structure.
The module defines only two low level macros, `openai:post` and `openai:get`.
These are used by higher level macros.
This structure makes it possible to follow the changes of the OpenAI API without the need of a new release of the macro module.

== Loading the module

The module is a so-called unsafe module.
Unsafe modules are not loaded by default by the core applications, like the command line version of Jamal, the asciidoc preprocessor or the Maven extension.
Other applications, like the Maven plugin implementation can be configured by the user to load any module, safe or unsafe.

The usual way to load an unsafe module is using the `maven:load` macro.
This macro needs the maven coordinates of the module to be loaded.
It also requires that the module is configured for the local Jamal installation.
The configuration tries to avoid arbitrary external modules loaded during Jamal document processing.
For more information about how to configure `maven:load` please read the link:../jamal-maven-load/README.adoc[documentation of the macro itself].


To load the macros use the following line in your Jamal document:

  {@maven:load com.javax0.jamal:jamal-openai:2.0.1-SNAPSHOT}\

The module defines macros on three levels.
The low level is the pure API access implemented with HTTP POST and GET.
The service level exposes the individual API calls and result the returning JSONS.
The high level macros are the most convenient to use.
They ask for the meaningful parameter and return the part of the resulting JSON that is the answer.

We have already loaded the macros into the jamal processor, but before using it they also need configuration.

== Configuration

Jamal and the different macro libraries follow the philosophy of convention over configuration.
Macro packages usually do not need configuration.
The OpenAI module is an exception, because you need to configure your OpenAI API key.
The API key is used to authenticate your requests to the OpenAI API.
You can get your API key from the OpenAI dashboard.

The macro library is designed to use the Jamal supported environment based configuration.
This configuration is looking for configuration values in:

* Java system properties,
* Environment variables,
* Configuration files.

in this order. The `OPENAI_API_KEY` environment variable is used to configure the API key.
When looking for Java system properties, Jamal replaces the `_` character with `.` and converts the name to lower case.
Thus, the environment variable `OPENAI_API_KEY` is converted to the Java system property `openai.api.key`.

There is another configuration key: `OPENAI_ORGANIZATION`.
This is the organization ID of your OpenAI account.
This configuration key is optional.

If the values are not specified in the system properties or environment variables, the configuration file is searched.
The configuration file is a properties file, `~/.jamal/settings.properties`.
My current configuration file looks like this:

  $ cat ~/.jamal/settings.properties
  openai.api.key=sk-GafL87XwJMDMluDGHf7bT3BlbkFZQf27uCVkIbkDbzLUn@Pt
  maven.load.include=com.javax0.jamal:jamal-openai:*

(Not a real key, though.)

== Low Level Macros

The module defines only two built-in macros:

* `openai:get`, and
* `openai:post`.

These macros are used to execute low level API calls to the OpenAI API.
You are not likely to use these macros directly in your document.
You will use the _Service_ macros or the __High Level__ macros defined in the `openai.jim` resource file.
The low level macros, however, are configurable through user defined macros.
These are general configuration possibilities, that also affect the higher level macros that invoke the low level macros.
In the following sections we will describe the configuration possibilities.

// adds the general description of the short form for the option


=== Define XXX `openai:url`

This option is used to define the URL, which is the target for the `POST` or `GET` requests.
The higher level macros configure this parameter.
You need to configure it only if you want to use the low level macros directly.

The name of the parameter is `openai:url` and it also has an alias `url`, which is the same without the `openai:` prefix.

The name of the parameter can be used defining a user defined macro, or used as option in the case of a boolean parameter. The alias cannot.
The short form is intended for brevity when used as a macro parameter.


=== Define Seed `openai:seed`

This parameter can define a seed value for the caching mechanism.
The caching mechanism is used to avoid unnecessary requests to the OpenAI API.
When a request to the API is answered a cache file is created.
The next time the same request will recognize that the response is already in the cache and will not send the request to the API.

The cache file name is a hash of the request.
It includes the URL, the parameters and the seed value.
The cache does not expire.
If you want to refresh a request you can delete the cache files or change the seed value.
Deleting the cache file may be difficult, because it is not trivial to find out the name of the cache file for the specific request.

In stead the seed value can be used to change the hash of the request.
The seed value is a string.

The name of the parameter is `openai:seed` and it also has an alias `seed`, which is the same without the `openai:` prefix.

The name of the parameter can be used defining a user defined macro, or used as option in the case of a boolean parameter. The alias cannot.
The short form is intended for brevity when used as a macro parameter.


You can set a global seed value using the line:

  {@define `openai:seed`=just some random text not used before to trigger new download}


=== Define Cache Location `openai:local`

The cache location is the `.openai` directory under the configured Jamal cache directory.
The default location is `~/.jamal/cache/.openai`.
This parameter will instruct the `GET` or `POST` macros to use document's directory as cache directory instead.
That way the cache directory `.openai` will join the document, and you can easily add to the git repository or to a ZIP file, whichever is your preferred way of maintaining your documents.
This way the cache files will also be stored separate from the caches responses of unrelated documents.

The name of the parameter is `openai:local` and it also has an alias `local`, which is the same without the `openai:` prefix.

The name of the parameter can be used defining a user defined macro, or used as option in the case of a boolean parameter. The alias cannot.
The short form is intended for brevity when used as a macro parameter.


It is usually a good idea setting this option globally for the whole document somewhere at the start of the document using the line

  {@options openai:local}

=== Sealing a Document `openai:sealed`

When you work on the document the rendering will invoke openai API calls many times.
You will change the requests a few times, and you will see the results.
When you settle and have the final version the result will always be the same and coming from the cache.

You can seal the document to ensure that no openai API calls gets modified and executed accidentally.
When the option `openai:sealed` is true the macros will throw an error if the response for a given request is not in the cache.

The name of the parameter is `openai:sealed` and it also has an alias `sealed`, which is the same without the `openai:` prefix.

The name of the parameter can be used defining a user defined macro, or used as option in the case of a boolean parameter. The alias cannot.
The short form is intended for brevity when used as a macro parameter.


It is usually a good idea setting this option globally for the whole document somewhere at the start of the document using the line

  {@options openai:sealed}

You can also reset this option for the different calls using the line

  {@options ~openai:sealed}


=== Define seal hash `openai:hash`

The option `openai:sealed` fails the document rendering if the cache file is missing.
Defining a hash value for a request will make the rendering fail even if there is a cached value, but is different from what the hash value imposes.

The typical use case is imagined as follows.
You edit a document, change the requests, and you see the results.
You seal the document when you are satisfied with the results.
You package the Jamal document along with the cache files and send it forward in the workflow.
You are responsible for the review of the openai generated text.

The next person edits parts of the file working on it, but should not change the openai requests.
This person can, however, edit the cache files, and the openai generated text manually.
It will change the document rendered version different from what you approved.

The manipulation can be investigated looking at the cache files, but it may not be trivial.
If you provide a hash value for the request changing the cache will need the change of the hash.
This will make the manipulation obvious.

The name of the parameter is `openai:hash` and it also has an alias `hash`, which is the same without the `openai:` prefix.

The name of the parameter can be used defining a user defined macro, or used as option in the case of a boolean parameter. The alias cannot.
The short form is intended for brevity when used as a macro parameter.


You can set the hash using the following line:

  {@define `openai:hash`=f4bf5bc6-7509e435-e10ab854-01aaad40-3a8e5269-92d71db9-f067d380-39ee3eb0}

The line above is an example of the hash value.
The hash value contains 8 parts of 8 hex characters separated by `-`.
You have to use at least six consecutive characters as a hash value from this string.

During rendering, when the hash value is defined, but wrong the error message will be

  The hash of the result is '22f1f8d7-028059c4-9f55984c-56430583-93c9fd5b-6482fb22-6a9af775-8da2c142' does not contain 'f4bf5bc6-7509e435-e10ab854-01aaad40-3a8e5269-92d71db9-f067d380-39ee3eb0'

You can then copy the hash value from the error message and use it in the definition of the hash value.

The value defined using `define` is used for the whole document.
The hash value, however, will be different for each request.
You have to add new `define` lines for each request.

If a specific request should be executed without checking the hash value, you should undefine the hash value macro using the line:

  {@undefine openai:hash}

Setting the hash value to an empty string does not work.

=== Accepting error responses `openai:fallible`

When the openai API returns an error response the rendering will fail.
Jamal tries to recover from error messages and list all the discoverable errors, but the rendering will fail.
When using the interactive IntelliJ editor it means that the "rendered" document will contain the macros and a huge load of stack trace information.

You can instruct the macros to ignore the error responses and return the error message instead.
It will not suppress errors, which are hard errors, like missing parameters or connection issues.
It will only suppress the errors, which are returned by the openai API or the error signaling an asynchronous pending response.

The name of the parameter is `openai:fallible` and it also has an alias `fallible`, which is the same without the `openai:` prefix.

The name of the parameter can be used defining a user defined macro, or used as option in the case of a boolean parameter. The alias cannot.
The short form is intended for brevity when used as a macro parameter.


You can set this option globally for the whole document using the line:

  {@options openai:fallible}

Note that result JSON structure is usually different from the legit response.
It means that in spite of using this option the documentum rendering may fail when using the returned JSON.
You can the `json:get` macro with alternative JSON pointers to handle error responses.
The first alternative should be the selection from the legit response.
The second should select from the error response.
It is usually just the name of the macro where the response is stored as JSON essentially selecting thw whole JSON.

=== Using the API Asynchronous `openai:asynch`

Sending a request to the openai and getting the response may take a while.
When rendering a document interactively it will freeze the editing environment.
To mitigate this situation you can use the option `openai:asynch`.
When this option is true the macros will return immediately with a progress information JSON.
Jamal will send the requests asynchronously and when the response arrives it will be stored in the cache.
Subsequent rendering will use the cached response.

The name of the parameter is `openai:asynch` and it also has an alias `asynch`, which is the same without the `openai:` prefix.

The name of the parameter can be used defining a user defined macro, or used as option in the case of a boolean parameter. The alias cannot.
The short form is intended for brevity when used as a macro parameter.


You should not use this option when rendering the document in a batch, like the command line.
It will generate output with the progress information JSONs instead of the actual responses.

To use this option only when the environment is interactive, like the IntelliJ Asciidoctor plugin you can use a line the following:

.sample from the SAMPLES/samples.jim file
[source]
----
{%#if /{%@env intellij.asciidoctor.plugin%}/
       {%@options openai:fallible openai:asynch%}/%}

----

This macro will set the fallible and asynch options when the environment is interactive.

== Loading OpenAI Service and High Level Macros

The Service and High Level macros are defined in the `openai.jim` resource file.

NOTE: Jamal include files have the extension `.jim` instead of `.jam` by convention.

Because the macros were loaded via the `maven:load` macro, the `openai.jim` file is not on the regular classpath.
Because of that the special form of the `res:` resource syntax has to be used naming a macro from the package where the resource file is.

  {@import res:`openai:get`openai.jim}

After you imported this file you can use the macros defined in it.

== Service Macros

Service macros use the built-in `openai:post` and `openai:get` macros to execute the API calls.
They are defined in the `openai.jim` resource file.

=== `openai:query_models`

This is the simplest macro.
It needs no parameters, and it returns a list of all available models.

  {@openai:query_models}

=== `openai:query_model(model)`


== High Level Macros
